{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFhMDyD-vXLs"
   },
   "source": [
    "NOTE: For the most up to date version of this notebook, please be sure to copy from this link:\n",
    " \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ByRi9d6_Yzu0nrEKArmLMLuMaZjYfygO#scrollTo=WgHANbxqWJPa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WgHANbxqWJPa"
   },
   "source": [
    "## **Training YOLOv3 object detection on a custom dataset**\n",
    "\n",
    "ðŸ’¡ Recommendation: [Open this blog post](https://blog.roboflow.ai/training-a-yolov3-object-detection-model-with-a-custom-dataset/) to continue.\n",
    "\n",
    "### **Overview**\n",
    "\n",
    "This notebook walks through how to train a YOLOv3 object detection model on your own dataset using Roboflow and Colab.\n",
    "\n",
    "In this specific example, we'll training an object detection model to recognize chess pieces in images. **To adapt this example to your own dataset, you only need to change one line of code in this notebook.**\n",
    "\n",
    "![Chess Example](https://i.imgur.com/nkjobw1.png)\n",
    "\n",
    "### **Our Data**\n",
    "\n",
    "Our dataset of 289 chess images (and 2894 annotations!) is hosted publicly on Roboflow [here](https://public.roboflow.ai/object-detection/chess-full).\n",
    "\n",
    "### **Our Model**\n",
    "\n",
    "We'll be training a YOLOv3 (You Only Look Once) model. This specific model is a one-shot learner, meaning each image only passes through the network once to make a prediction, which allows the architecture to be very performant, viewing up to 60 frames per second in predicting against video feeds.\n",
    "\n",
    "The GitHub repo containing the majority of the code we'll use is available [here](https://github.com/roboflow-ai/keras-yolo3.git).\n",
    "\n",
    "### **Training**\n",
    "\n",
    "Google Colab provides free GPU resources. Click \"Runtime\" â†’ \"Change runtime type\" â†’ Hardware Accelerator dropdown to \"GPU.\"\n",
    "\n",
    "Colab does have memory limitations, and notebooks must be open in your browser to run. Sessions automatically clear themselves after 24 hours.\n",
    "\n",
    "### **Inference**\n",
    "\n",
    "We'll leverage the `python_video.py` script to produce predictions. Arguments are specified below.\n",
    "\n",
    "It's recommended that you expand the left-hand panel to view this notebook's Table of contents, Code Snippets, and Files. \n",
    "\n",
    "![Expand Colab](https://i.imgur.com/r8kWzIv.png \"Click here\")\n",
    "\n",
    "Then, click \"Files.\" You'll see files appear here as we work through the notebook.\n",
    "\n",
    "\n",
    "### **About**\n",
    "\n",
    "[Roboflow](https://roboflow.ai) makes managing, preprocessing, augmenting, and versioning datasets for computer vision seamless.\n",
    "\n",
    "Developers reduce 50% of their boilerplate code when using Roboflow's workflow, save training time, and increase model reproducibility.\n",
    "\n",
    "#### ![Roboflow Workmark](https://i.imgur.com/WHFqYSJ.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aHNPC6kwbKAL"
   },
   "source": [
    "## Setup our environment\n",
    "\n",
    "First, we'll install the version of Keras our YOLOv3 implementation calls for and verify it installs corrects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "R6DNWhOEbGB6",
    "outputId": "b7d62815-a456-43b7-8786-879641e610b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco_annotation.py  LICENSE                train_bottleneck.py  yolo.py\n",
      "convert.py          \u001b[0m\u001b[01;34mmodel_data\u001b[0m/            train.py             yolov3.cfg\n",
      "darknet53.cfg       README.md              Tutorial.ipynb       yolov3-tiny.cfg\n",
      "\u001b[01;34mfont\u001b[0m/               Roboflow_Yolov3.ipynb  voc_annotation.py    yolov3.weights\n",
      "kmeans.py           \u001b[01;34msample_data\u001b[0m/           \u001b[01;34myolo3\u001b[0m/               yolo_video.py\n"
     ]
    }
   ],
   "source": [
    "# show the contents of our repo\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I--RqDmpwqmv"
   },
   "source": [
    "## Get our training data from Roboflow\n",
    "\n",
    "Next, we need to add our data from Roboflow into our environment.\n",
    "\n",
    "Our dataset, with annotations, is [here](https://public.roboflow.ai/object-detection/chess-full).\n",
    "\n",
    "Here's how to bring those images from Roboflow to Colab:\n",
    "\n",
    "1. Visit this [link](https://public.roboflow.ai/object-detection/chess-full).\n",
    "2. Click the \"416x416auto-orient\" under Downloads.\n",
    "3. On the dataset detail page, select \"Download\" in the upper right-hand corner.\n",
    "4. If you are not signed in, you will be prompted to create a free account (sign in with GitHub or email), and redirected to the dataset page to Download.\n",
    "5. On the download popup, select the YOLOv3 Keras option **and** the \"Show download `code`\". \n",
    "6. Copy the code snippet Roboflow generates for you, and paste it in the next cell.\n",
    "\n",
    "This is the download menu you want (from step 5):\n",
    "#### ![Download Menu](https://i.imgur.com/KW2PyQO.png)\n",
    "\n",
    "The top code snippet is the one you want to copy (from step 6) and paste in the next notebook cell:\n",
    "### ![Code Snippet](https://i.imgur.com/qzJckWR.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6AmSSTFFWud7"
   },
   "source": [
    "**This cell below is only one you need to change to have YOLOv3 train on your own Roboflow dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "0nclkjonbT25",
    "outputId": "a913b741-3f7f-4c4d-e0fc-77599b9a628c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   892  100   892    0     0    305      0  0:00:02  0:00:02 --:--:--   304\n",
      "100 26.3M  100 26.3M    0     0  4941k      0  0:00:05  0:00:05 --:--:-- 11.0M\n",
      "Archive:  roboflow.zip\n",
      " extracting: test/heon_IMG_0616_JPG.rf.0a38389579bae79358a0f1c1a202148b.jpg  \n",
      " extracting: train/heon_IMG_0618_JPG.rf.07ecc17611eee3db067beeb062bb6b7c.jpg  \n",
      " extracting: test/heon_IMG_0530_JPG.rf.9d46ea825abf55caf1ecea3a89b08aca.jpg  \n",
      " extracting: test/heon_IMG_0653_JPG.rf.d3371cfdf04eafadda1bc35c0b84b328.jpg  \n",
      " extracting: test/heon_IMG_0625_JPG.rf.382581c3504cb182c4b4afe05d0674ff.jpg  \n",
      " extracting: test/heon_IMG_0680_JPG.rf.09def9112d00b15d5376fe916917b394.jpg  \n",
      " extracting: test/heon_IMG_0575_JPG.rf.5963aaa23604dad7a5ce08f1052cd4e3.jpg  \n",
      " extracting: test/heon_IMG_0675_JPG.rf.eb7127ef2cdb08a4a99c43d8294fa23e.jpg  \n",
      " extracting: train/heon_IMG_0624_JPG.rf.055306bb80e0b99433c6b849fdfbf67d.jpg  \n",
      " extracting: train/heon_IMG_0659_JPG.rf.01a778de995d19e652c3c02e52a9c39d.jpg  \n",
      " extracting: test/heon_IMG_0521_JPG.rf.5a6cac64d6004050e8d0ea34352e39cc.jpg  \n",
      " extracting: test/heon_IMG_0670_JPG.rf.c2862ba1ffd04665deab0fcdb7284538.jpg  \n",
      " extracting: test/heon_IMG_0635_JPG.rf.5d7b682e4175f6467ea6ad7986c15d26.jpg  \n",
      " extracting: test/heon_IMG_0617_JPG.rf.801ff47bb676e5f9521e9fa24142b225.jpg  \n",
      " extracting: test/heon_IMG_0613_JPG.rf.4db1cf1607b2cd7f973175722a895583.jpg  \n",
      " extracting: test/heon_IMG_0647_JPG.rf.e971ef9f619f24309196a9fbb8f08f29.jpg  \n",
      " extracting: train/heon_IMG_0685_JPG.rf.115747f0100012bfd2e5e5712508e743.jpg  \n",
      " extracting: train/heon_IMG_0626_JPG.rf.0db07729ff5d0abdf9829f7146868523.jpg  \n",
      " extracting: test/heon_IMG_0573_JPG.rf.9e342b83519399fbc778ebbba6e4f0cf.jpg  \n",
      " extracting: test/heon_IMG_0725_JPG.rf.f4aa83706955dbdd1e5bd50c48260312.jpg  \n",
      " extracting: test/heon_IMG_0544_JPG.rf.2a7ac67b7fa7a680472963fd20939a24.jpg  \n",
      " extracting: test/heon_IMG_0524_JPG.rf.255f5f011423ad47d0a57d627fb874d4.jpg  \n",
      " extracting: test/heon_IMG_0520_JPG.rf.1f3a1f10586df053254c61f3247298f1.jpg  \n",
      " extracting: train/heon_IMG_0579_JPG.rf.0914b76902706aa03fdc7d5c2055983d.jpg  \n",
      " extracting: test/heon_IMG_0587_JPG.rf.b1867145d8c81795cc5c30689c8fd94a.jpg  \n",
      " extracting: train/heon_IMG_0547_JPG.rf.14c577948a1e53233fcad8cb2f592125.jpg  \n",
      " extracting: train/heon_IMG_0736_JPG.rf.1c3f7b92346973e07fe9c19df8cd8764.jpg  \n",
      " extracting: train/heon_IMG_0610_JPG.rf.270bf8e5a513f57917506b207c799a5e.jpg  \n",
      " extracting: train/heon_IMG_0531_JPG.rf.27bbe28006293232c6dc1b6b6b7f5977.jpg  \n",
      " extracting: train/heon_IMG_0594_JPG.rf.18267bb33089347b744ef0350dad752c.jpg  \n",
      " extracting: train/heon_IMG_0601_JPG.rf.2bc3708be72c401f7b9f1bd0afcc2ada.jpg  \n",
      " extracting: train/heon_IMG_0551_JPG.rf.2086ce1895e162689c71ade7a4402bf5.jpg  \n",
      " extracting: train/heon_IMG_0728_JPG.rf.15de0840ed98ca021e78cbb240e8d493.jpg  \n",
      " extracting: train/heon_IMG_0655_JPG.rf.1f2903cb6d3e73bc9081ae0c2fbd217d.jpg  \n",
      " extracting: train/heon_IMG_0588_JPG.rf.32f2cba2ffa7a296496af616742cd847.jpg  \n",
      " extracting: train/heon_IMG_0555_JPG.rf.33ed8e53807854825acb52f9f6ea4703.jpg  \n",
      " extracting: train/heon_IMG_0628_JPG.rf.3dacb61c3e86416a5911a283e4e532c4.jpg  \n",
      " extracting: train/heon_IMG_0578_JPG.rf.3f510bd45d9a53be074aca08049270b2.jpg  \n",
      " extracting: train/heon_IMG_0558_JPG.rf.344a48b8e8bdc40cbf7cbf826b99bc8b.jpg  \n",
      " extracting: train/heon_IMG_0560_JPG.rf.324157c07bd469788f064d70b1c5365f.jpg  \n",
      " extracting: train/heon_IMG_0652_JPG.rf.3e8c552c36b8f41d03bd884778b7aebb.jpg  \n",
      " extracting: train/heon_IMG_0681_JPG.rf.37c4563d158a9b1eba284ac931486956.jpg  \n",
      " extracting: train/heon_IMG_0723_JPG.rf.2cf6f8a5f64973fb45dc793ff5c1c677.jpg  \n",
      " extracting: train/heon_IMG_0683_JPG.rf.425dc9ab950776855b9f74f8d942ef14.jpg  \n",
      " extracting: train/heon_IMG_0566_JPG.rf.367639a1d156419701de1f4bd2392bd0.jpg  \n",
      " extracting: train/heon_IMG_0687_JPG.rf.223b02dac6da53ce0bac5ece17642b75.jpg  \n",
      " extracting: train/heon_IMG_0612_JPG.rf.4056d25d1748b444b9be1d645ef17998.jpg  \n",
      " extracting: train/heon_IMG_0692_JPG.rf.2bc5cb7ed9087d0a980dd46877eeb5e0.jpg  \n",
      " extracting: train/heon_IMG_0662_JPG.rf.3ff22cde91bce1bacb10911d86b66af4.jpg  \n",
      " extracting: train/heon_IMG_0597_JPG.rf.33f8cb643a669b3ff2f744cb9111ecb1.jpg  \n",
      " extracting: train/heon_IMG_0690_JPG.rf.4bc0062a7589308d21047b5086cb2952.jpg  \n",
      " extracting: train/heon_IMG_0590_JPG.rf.718858967e8d70b3c44c9bf17c58d2dc.jpg  \n",
      " extracting: train/heon_IMG_0602_JPG.rf.478bd431fc5c8344b64b3579ccc9e42c.jpg  \n",
      " extracting: train/heon_IMG_0586_JPG.rf.4ea33afc0d2239be486e15fd757644ad.jpg  \n",
      " extracting: train/heon_IMG_0663_JPG.rf.555026d62023ba33caa77192f1ee0090.jpg  \n",
      " extracting: train/heon_IMG_0614_JPG.rf.504d99915c4bbb1a905d04f63ab0b873.jpg  \n",
      " extracting: train/heon_IMG_0669_JPG.rf.4f2d25e1eb9edc4b332c4d4d19a2b46a.jpg  \n",
      " extracting: train/heon_IMG_0591_JPG.rf.66490b77514f1cfd4c99ae8d9a216e96.jpg  \n",
      " extracting: train/heon_IMG_0734_JPG.rf.5d086e505efea32199bda5b0d87797a4.jpg  \n",
      " extracting: train/heon_IMG_0646_JPG.rf.67de370ba0d064c536e5eb5bb5c70858.jpg  \n",
      " extracting: train/heon_IMG_0559_JPG.rf.4f159543bcbc63362d8deef8ae60a6c4.jpg  \n",
      " extracting: train/heon_IMG_0611_JPG.rf.5ced443931fd896fa1f4b31af09ca91f.jpg  \n",
      " extracting: train/heon_IMG_0746_JPG.rf.5673f48d34275390675539bdc8c4211d.jpg  \n",
      " extracting: train/heon_IMG_0609_JPG.rf.6af2e6f505db3c46da17a44be49c196f.jpg  \n",
      " extracting: train/heon_IMG_0650_JPG.rf.609a0a93b4e9949e0930b80b56beb236.jpg  \n",
      " extracting: train/heon_IMG_0689_JPG.rf.60624cde078f2ab56507c929bbce9e1a.jpg  \n",
      " extracting: train/heon_IMG_0568_JPG.rf.62aababee0d80ce94d72ff28ff6f1ffd.jpg  \n",
      " extracting: train/heon_IMG_0667_JPG.rf.59f1fdb07cc0f68308d9b1b2353a9528.jpg  \n",
      " extracting: train/heon_IMG_0557_JPG.rf.67bd9dbf1421fc617e76f50d8d1695a8.jpg  \n",
      " extracting: train/heon_IMG_0540_JPG.rf.5ef7de2971c003ab8e7cb8667a3aff6c.jpg  \n",
      " extracting: train/heon_IMG_0593_JPG.rf.7142dc37bfef93ecc8340fe9b82f6132.jpg  \n",
      " extracting: train/heon_IMG_0567_JPG.rf.5a72f367c9afb5d0af537e42c03cd781.jpg  \n",
      " extracting: train/heon_IMG_0545_JPG.rf.52f13fb2dd164d495a956d17fadc4500.jpg  \n",
      " extracting: train/heon_IMG_0744_JPG.rf.71f614aa08577f7fee9dbdb0b0142b44.jpg  \n",
      " extracting: train/heon_IMG_0572_JPG.rf.4f64a1fe8c6c826d8842d976e1314c6a.jpg  \n",
      " extracting: train/heon_IMG_0729_JPG.rf.8bc48fc9dd65b218ff45fc4f74b678ba.jpg  \n",
      " extracting: train/heon_IMG_0722_JPG.rf.84f3d2cd3183c61b3716b65c3bc7e672.jpg  \n",
      " extracting: train/heon_IMG_0656_JPG.rf.7c0e3c325e95275380be0d78156f7369.jpg  \n",
      " extracting: train/heon_IMG_0623_JPG.rf.805880719c08fe1b01db3c60562546a0.jpg  \n",
      " extracting: train/heon_IMG_0634_JPG.rf.8433cbb8c4c4398fbc457395c6579cd3.jpg  \n",
      " extracting: train/heon_IMG_0529_JPG.rf.95031be7586007e341c8dfc792c8b042.jpg  \n",
      " extracting: train/heon_IMG_0536_JPG.rf.8473bae023642862f418b9adb7066099.jpg  \n",
      " extracting: train/heon_IMG_0595_JPG.rf.745f6c4b16533db65d7960aac8d55036.jpg  \n",
      " extracting: train/heon_IMG_0553_JPG.rf.7eaa7cb02c32171c8dc6560d3585e7a1.jpg  \n",
      " extracting: train/heon_IMG_0664_JPG.rf.935c09e81cdcfd83df502c9772d06571.jpg  \n",
      " extracting: train/heon_IMG_0737_JPG.rf.856a39a636619c5bd03d9aa226dc25e6.jpg  \n",
      " extracting: train/heon_IMG_0627_JPG.rf.94530e960f0e83dd01d9d705afc2f7c1.jpg  \n",
      " extracting: train/heon_IMG_0630_JPG.rf.96645848365cde734feffe1fff2abfcc.jpg  \n",
      " extracting: train/heon_IMG_0644_JPG.rf.8310529eacfe97fcbc3d605defdd4fec.jpg  \n",
      " extracting: train/heon_IMG_0684_JPG.rf.885131c97c334050e4b21ba807b83b93.jpg  \n",
      " extracting: train/heon_IMG_0631_JPG.rf.88b33ddfd1f54a3938ec5bbfbaa67455.jpg  \n",
      " extracting: train/heon_IMG_0645_JPG.rf.93143018d35aa24511991df866c517c5.jpg  \n",
      " extracting: train/heon_IMG_0605_JPG.rf.99b80d98ab5324e8629618b4378c8780.jpg  \n",
      " extracting: train/heon_IMG_0657_JPG.rf.86347a2f375f78f937b7c75ee227fbf1.jpg  \n",
      " extracting: train/heon_IMG_0697_JPG.rf.872eae0ef59f1dec4739a40f0d35e84c.jpg  \n",
      " extracting: train/heon_IMG_0632_JPG.rf.99eff5c3ffc174e9f0b34442e0df92fc.jpg  \n",
      " extracting: train/heon_IMG_0688_JPG.rf.96b6d6b5f7de425981d253bdd9b4a126.jpg  \n",
      " extracting: train/heon_IMG_0556_JPG.rf.9c9224d5cdfdbca70eee82fe5e2ab19f.jpg  \n",
      " extracting: train/heon_IMG_0615_JPG.rf.9521d1c7308c92a38dbfee394a79f6cb.jpg  \n",
      " extracting: train/heon_IMG_0674_JPG.rf.78ed8d8799e8d03b62763b481c019cb6.jpg  \n",
      " extracting: train/heon_IMG_0550_JPG.rf.9e1aa6c985bb850009ae8bd2ed0781e1.jpg  \n",
      " extracting: train/heon_IMG_0548_JPG.rf.a1aa1b7a13b75f2e82cefc3e42b5b8bf.jpg  \n",
      " extracting: train/heon_IMG_0691_JPG.rf.9e50d10eb2ecdec970e75425ab7fabb2.jpg  \n",
      " extracting: train/heon_IMG_0738_JPG.rf.af3413888e091ed5871c812ed5eed4fd.jpg  \n",
      " extracting: train/heon_IMG_0720_JPG.rf.bed68307be79329a53b5267ff895cc37.jpg  \n",
      " extracting: train/heon_IMG_0666_JPG.rf.aee22f15564d2fe8ba82b68b195fac3e.jpg  \n",
      " extracting: train/heon_IMG_0726_JPG.rf.abed8adebc0723f160f0b42818a8d772.jpg  \n",
      " extracting: train/heon_IMG_0571_JPG.rf.b7ea0d6cf7f3f9d749b10913ccd7371a.jpg  \n",
      " extracting: train/heon_IMG_0570_JPG.rf.b42212eff35ffd623804faf202fbf893.jpg  \n",
      " extracting: train/heon_IMG_0543_JPG.rf.b83f5470526c5802cf1972ac0deacbe9.jpg  \n",
      " extracting: train/heon_IMG_0565_JPG.rf.ab9420c5357f1a3413dad8d89d366d24.jpg  \n",
      " extracting: train/heon_IMG_0519_JPG.rf.a7cc4dc8c1f0e4a93b2dc20a56e8bf1c.jpg  \n",
      " extracting: train/heon_IMG_0596_JPG.rf.b5adeecc70d7f2cc196db32c4abff553.jpg  \n",
      " extracting: train/heon_IMG_0686_JPG.rf.a0eeb148ab2776879a1603d33fa0b9c5.jpg  \n",
      " extracting: train/heon_IMG_0661 2_JPG.rf.bdcdda934029560d13c75ceb3d937dd6.jpg  \n",
      " extracting: train/heon_IMG_0730_JPG.rf.bd7c92fe671a84d885081a41ba9cbb32.jpg  \n",
      " extracting: train/heon_IMG_0546_JPG.rf.a270a8a0cd8f01fb3f6ae362e068c3ba.jpg  \n",
      " extracting: train/heon_IMG_0727_JPG.rf.bf9f581e656e1f68c9cc0eed8de960ab.jpg  \n",
      " extracting: train/heon_IMG_0604_JPG.rf.a477fb3f459289497479bead5b67b7af.jpg  \n",
      " extracting: train/heon_IMG_0589_JPG.rf.b32d9d276f631765d3a53408bfdd0649.jpg  \n",
      " extracting: train/heon_IMG_0620_JPG.rf.b0a4f50b8fbb995cc6243b8276bde607.jpg  \n",
      " extracting: train/heon_IMG_0525_JPG.rf.b11b7976a74d7ca79dbd483b0ccef50a.jpg  \n",
      " extracting: train/heon_IMG_0643_JPG.rf.afd521a657ce06da6c47ab11baaf5b46.jpg  \n",
      " extracting: train/heon_IMG_0607_JPG.rf.a3b5c45b40bf4dfe6354f60658e9d5d6.jpg  \n",
      " extracting: train/heon_IMG_0526_JPG.rf.c31f330389ac146a96a0ae5fbed9f8a6.jpg  \n",
      " extracting: train/heon_IMG_0603_JPG.rf.cebc349c76a94aa7b5ec56e11dd785b0.jpg  \n",
      " extracting: train/heon_IMG_0574_JPG.rf.c4a5d8fb6f29e35e2911ab871fb76a64.jpg  \n",
      " extracting: train/heon_IMG_0694_JPG.rf.ed597277ac0fc8dc7ab15707a1329dc4.jpg  \n",
      " extracting: train/heon_IMG_0537_JPG.rf.eca677ee21c51b9eb459655e365edf5b.jpg  \n",
      " extracting: train/heon_IMG_0541_JPG.rf.e2e64f6219f906772ab30b17103efbbe.jpg  \n",
      " extracting: train/heon_IMG_0745_JPG.rf.de04e98f7401c0a512cb8c9d4d6db8eb.jpg  \n",
      " extracting: train/heon_IMG_0696_JPG.rf.e9ab910cf7529dcf8a1406408120086e.jpg  \n",
      " extracting: train/heon_IMG_0651_JPG.rf.c72877f835b753f0a8a2d8810f5c2899.jpg  \n",
      " extracting: train/heon_IMG_0678_JPG.rf.e5c546b3314a132123ac66e39f9cf760.jpg  \n",
      " extracting: train/heon_IMG_0668_JPG.rf.d5c384c3bb8be1de7d98a324a7151e34.jpg  \n",
      " extracting: train/heon_IMG_0534_JPG.rf.cfcf9a9d64deeef139c68358b5d86cbe.jpg  \n",
      " extracting: train/heon_IMG_0733_JPG.rf.e7ecd32aae40ab3af4223f84975e7766.jpg  \n",
      " extracting: train/heon_IMG_0569_JPG.rf.c7cd9c08924ba2e7b52d7564fa9c695c.jpg  \n",
      " extracting: train/heon_IMG_0554_JPG.rf.c822ed329ec3c500611c1d2130f8bf65.jpg  \n",
      " extracting: train/heon_IMG_0693_JPG.rf.d6f54aef72399005b79e3ca6d8f3d991.jpg  \n",
      " extracting: train/heon_IMG_0672_JPG.rf.cf466b0750fdc99a19a6fc73bccb3723.jpg  \n",
      " extracting: train/heon_IMG_0606_JPG.rf.eec12f7fbb03e700074d7c6ea4260733.jpg  \n",
      " extracting: train/heon_IMG_0599_JPG.rf.f8ad60d6b05f9166f3a063fef0033642.jpg  \n",
      " extracting: train/heon_IMG_0633_JPG.rf.e0b0d8b246f3f1f6b62ac93078081321.jpg  \n",
      " extracting: train/heon_IMG_0629_JPG.rf.dfaddcb2759a34658943bd23af6608e3.jpg  \n",
      " extracting: train/heon_IMG_0671_JPG.rf.df33241cdd8cc559023d4494cebc4b4c.jpg  \n",
      " extracting: train/heon_IMG_0739_JPG.rf.efccd7e5cc6c5d219d779a1ff48730a7.jpg  \n",
      " extracting: train/heon_IMG_0741_JPG.rf.c7b468c35c155d249931085fd8cf6249.jpg  \n",
      " extracting: train/heon_IMG_0642_JPG.rf.db4757a0e288a91413f358cdf903f321.jpg  \n",
      " extracting: train/heon_IMG_0740_JPG.rf.f4e0352205b44f888c81974a523666d6.jpg  \n",
      " extracting: valid/heon_IMG_0679_JPG.rf.0e8af5961049105a93efe0780bbab6bd.jpg  \n",
      " extracting: train/heon_IMG_0658_JPG.rf.ff45ab116d828d02ac2c589c2881bc6a.jpg  \n",
      " extracting: valid/heon_IMG_0695_JPG.rf.10bb70c2667e02fd97ca67630ff0d843.jpg  \n",
      " extracting: valid/heon_IMG_0552_JPG.rf.14d40e3a5f2dcbb55939629243fac81f.jpg  \n",
      " extracting: valid/heon_IMG_0721_JPG.rf.3fffedd9adcdb09bf43c8441d9ecdfb4.jpg  \n",
      " extracting: valid/heon_IMG_0665_JPG.rf.262192c840a3676f59d15dd298582e0e.jpg  \n",
      " extracting: valid/heon_IMG_0676_JPG.rf.0c364d88175bb586a59a908c7957eb18.jpg  \n",
      " extracting: valid/heon_IMG_0732_JPG.rf.2cd0eb6b234fb53c232b26b5b04e1a4c.jpg  \n",
      " extracting: valid/heon_IMG_0592_JPG.rf.549c89cd8e13bdb936e3fce671e3e01b.jpg  \n",
      " extracting: valid/heon_IMG_0581_JPG.rf.499554191003eed484cf98e8e74dd1b6.jpg  \n",
      " extracting: train/heon_IMG_0677_JPG.rf.fc3c854832154de7b50118d3728a7bef.jpg  \n",
      " extracting: valid/heon_IMG_0649_JPG.rf.2e1c020422c651357abf8b66f79777b6.jpg  \n",
      " extracting: valid/heon_IMG_0542_JPG.rf.43d99c8008f5bf50ccc972ec88d019dd.jpg  \n",
      " extracting: valid/heon_IMG_0608_JPG.rf.26aeffea070c7c6ef3026f8b510327e0.jpg  \n",
      " extracting: valid/heon_IMG_0673_JPG.rf.175462c359dcdca24cc7794e1b1e712a.jpg  \n",
      " extracting: train/heon_IMG_0743_JPG.rf.fb4114e3d3ef5b87bde39d4e2845b03e.jpg  \n",
      " extracting: train/heon_IMG_0699_JPG.rf.fe07fd8896589d889957cddd4231fffa.jpg  \n",
      " extracting: valid/heon_IMG_0561_JPG.rf.2c28d258c1d9bb5f5dc7f9ce3bd0a421.jpg  \n",
      " extracting: valid/heon_IMG_0735_JPG.rf.348debf58f21780f5414e373328464d0.jpg  \n",
      " extracting: valid/heon_IMG_0621_JPG.rf.0e76b657b50e864bdfbfb84d5f574dc9.jpg  \n",
      " extracting: valid/heon_IMG_0549_JPG.rf.29888a2b294b1d0276a968e22c703153.jpg  \n",
      " extracting: valid/heon_IMG_0600_JPG.rf.5905c6b5ca78af7408c49cf4ce855fa9.jpg  \n",
      " extracting: valid/heon_IMG_0563_JPG.rf.237c13d08b5fdb383aaf216d2c939cb6.jpg  \n",
      " extracting: valid/heon_IMG_0538 2_JPG.rf.66f97f1f3dd754bb9248fd626557a049.jpg  \n",
      " extracting: valid/heon_IMG_0682_JPG.rf.65518b522b743631af569e16713b3530.jpg  \n",
      " extracting: valid/heon_IMG_0654_JPG.rf.6831466b98295c8d9c5122d1ebd74f73.jpg  \n",
      " extracting: valid/heon_IMG_0577_JPG.rf.856178c95ac05974b8ea6563c344b880.jpg  \n",
      " extracting: valid/heon_IMG_0576_JPG.rf.a299632b730b320181ab98a2838f9cdd.jpg  \n",
      " extracting: valid/heon_IMG_0660_JPG.rf.aeb4b0c98376d6b20e9b0b3eb7b24e16.jpg  \n",
      " extracting: valid/heon_IMG_0622_JPG.rf.f3bf48feea4a0534894e7a82cb94e08d.jpg  \n",
      " extracting: valid/heon_IMG_0532_JPG.rf.fba4726a36ff48eb754b902a11024a61.jpg  \n",
      " extracting: valid/heon_IMG_0648_JPG.rf.76a3c0b3e34484569909f15ae9138d73.jpg  \n",
      " extracting: valid/heon_IMG_0523_JPG.rf.b78be2c1310bfa0253fb365c38e1a059.jpg  \n",
      " extracting: valid/heon_IMG_0564_JPG.rf.75eb3b7c6d9c50236ed97c4c13df6253.jpg  \n",
      " extracting: valid/heon_IMG_0742_JPG.rf.7ffdf27f246013a93422e6519b6f4177.jpg  \n",
      " extracting: valid/heon_IMG_0522_JPG.rf.f54b78074a78ad805e1e9f6da4397f7d.jpg  \n",
      " extracting: valid/heon_IMG_0518_JPG.rf.76fa9eb304201e6ce4f768ec154dfcf0.jpg  \n",
      " extracting: valid/heon_IMG_0698_JPG.rf.f5091aeb0c4e19f26141554b213cb0a6.jpg  \n",
      " extracting: valid/heon_IMG_0580_JPG.rf.9b2b69c21331f56b0aae06a4829bb355.jpg  \n",
      " extracting: valid/heon_IMG_0731_JPG.rf.80739daad401a1575df211ce1ce0a869.jpg  \n",
      " extracting: valid/heon_IMG_0598_JPG.rf.cf4aab6756c693b1dba75cfc29f5c5e5.jpg  \n",
      " extracting: valid/heon_IMG_0619_JPG.rf.ab88790167c112b880988a3af61c0c8a.jpg  \n",
      " extracting: valid/heon_IMG_0724_JPG.rf.cd769596d0f51fbd582432fbc2cbf84f.jpg  \n",
      " extracting: test/_annotations.txt   \n",
      " extracting: test/_classes.txt       \n",
      " extracting: train/_annotations.txt  \n",
      " extracting: train/_classes.txt      \n",
      " extracting: valid/_annotations.txt  \n",
      " extracting: valid/_classes.txt      \n",
      " extracting: README.roboflow.txt     \n"
     ]
    }
   ],
   "source": [
    "# Paste Roboflow code from snippet here from above to here! eg !curl -L https://app.roboflow.ai/ds/eOSXbt7KWu?key=YOURKEY | jar -x\n",
    "!curl -L https://app.roboflow.ai/ds/xNUivrya90?key=sy2L6PqNff > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "v1PagPopmUIG",
    "outputId": "4652fa36-a7ac-44d6-d08d-6868fbbaace9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eduard/notebooks/tensorflow-notebook/workspace/09MAIR-Trabajo_Fin_de_Master/test/sample_data/train\n"
     ]
    }
   ],
   "source": [
    "# change directory into our export folder from Roboflow\n",
    "%ls sample_data/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "CQASf1hzmxE7",
    "outputId": "71b77f85-b57b-4db7-c540-84a25ebe3d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coco_annotation.py  LICENSE              Tutorial.ipynb     yolov3-tiny.cfg\n",
      "convert.py          \u001b[0m\u001b[01;34mmodel_data\u001b[0m/          voc_annotation.py  yolo_video.py\n",
      "darknet53.cfg       README.md            \u001b[01;34myolo3\u001b[0m/\n",
      "\u001b[01;34mfont\u001b[0m/               train_bottleneck.py  yolo.py\n",
      "kmeans.py           train.py             yolov3.cfg\n"
     ]
    }
   ],
   "source": [
    "# show that all our images, _annotations.txt, and _classes.txt made it to our root directory\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WvzqgP92W7bt"
   },
   "source": [
    "## Set up and train our model\n",
    "\n",
    "Next, we'll download pre-trained weighs weights from DarkNet, set up our YOLOv3 architecture with those pre-trained weights, and initiate training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "fJzW08g2VlwD",
    "outputId": "ad469e73-2dbe-4e8b-bbbf-19c80fdb99a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-02 17:05:55--  https://pjreddie.com/media/files/yolov3.weights\n",
      "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
      "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 248007048 (237M) [application/octet-stream]\n",
      "Saving to: â€˜yolov3.weightsâ€™\n",
      "\n",
      "yolov3.weights      100%[===================>] 236,52M   963KB/s    in 6m 58s  \n",
      "\n",
      "2020-05-02 17:12:55 (580 KB/s) - â€˜yolov3.weightsâ€™ saved [248007048/248007048]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download our DarkNet weights \n",
    "!wget https://pjreddie.com/media/files/yolov3.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mub8GJMBVluA",
    "outputId": "266ebd55-0150-45da-d813-0f8b46f5d5c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights.\n",
      "Weights Header:  0 2 0 [32013312]\n",
      "Parsing Darknet config.\n",
      "Creating Keras model.\n",
      "Parsing section net_0\n",
      "Parsing section convolutional_0\n",
      "conv2d bn leaky (3, 3, 3, 32)\n",
      "2020-05-02 17:17:44.072665: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-05-02 17:17:44.099186: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-05-02 17:17:44.099573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 2060 computeCapability: 7.5\n",
      "coreClock: 1.68GHz coreCount: 30 deviceMemorySize: 5.79GiB deviceMemoryBandwidth: 312.97GiB/s\n",
      "2020-05-02 17:17:44.099734: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-05-02 17:17:44.101056: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-05-02 17:17:44.102107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-05-02 17:17:44.102358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-05-02 17:17:44.103752: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-05-02 17:17:44.104529: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-05-02 17:17:44.107553: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-05-02 17:17:44.107705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-05-02 17:17:44.108138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-05-02 17:17:44.108466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "2020-05-02 17:17:44.108707: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\n",
      "2020-05-02 17:17:44.134473: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3411500000 Hz\n",
      "2020-05-02 17:17:44.134975: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558a061aec20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-05-02 17:17:44.135001: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-05-02 17:17:44.135190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-05-02 17:17:44.135680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 2060 computeCapability: 7.5\n",
      "coreClock: 1.68GHz coreCount: 30 deviceMemorySize: 5.79GiB deviceMemoryBandwidth: 312.97GiB/s\n",
      "2020-05-02 17:17:44.135727: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-05-02 17:17:44.135752: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2020-05-02 17:17:44.135775: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2020-05-02 17:17:44.135799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2020-05-02 17:17:44.135820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2020-05-02 17:17:44.135844: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2020-05-02 17:17:44.135866: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-05-02 17:17:44.135939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-05-02 17:17:44.136446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-05-02 17:17:44.136894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "2020-05-02 17:17:44.136935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2020-05-02 17:17:44.234154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-05-02 17:17:44.234196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \n",
      "2020-05-02 17:17:44.234207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \n",
      "2020-05-02 17:17:44.234386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-05-02 17:17:44.234806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-05-02 17:17:44.235177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2020-05-02 17:17:44.235524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4796 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5)\n",
      "2020-05-02 17:17:44.237151: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558a09da5bf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2020-05-02 17:17:44.237174: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2060, Compute Capability 7.5\n",
      "Parsing section convolutional_1\n",
      "conv2d bn leaky (3, 3, 32, 64)\n",
      "Parsing section convolutional_2\n",
      "conv2d bn leaky (1, 1, 64, 32)\n",
      "Parsing section convolutional_3\n",
      "conv2d bn leaky (3, 3, 32, 64)\n",
      "Parsing section shortcut_0\n",
      "Parsing section convolutional_4\n",
      "conv2d bn leaky (3, 3, 64, 128)\n",
      "Parsing section convolutional_5\n",
      "conv2d bn leaky (1, 1, 128, 64)\n",
      "Parsing section convolutional_6\n",
      "conv2d bn leaky (3, 3, 64, 128)\n",
      "Parsing section shortcut_1\n",
      "Parsing section convolutional_7\n",
      "conv2d bn leaky (1, 1, 128, 64)\n",
      "Parsing section convolutional_8\n",
      "conv2d bn leaky (3, 3, 64, 128)\n",
      "Parsing section shortcut_2\n",
      "Parsing section convolutional_9\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section convolutional_10\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_11\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section shortcut_3\n",
      "Parsing section convolutional_12\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_13\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section shortcut_4\n",
      "Parsing section convolutional_14\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_15\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section shortcut_5\n",
      "Parsing section convolutional_16\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_17\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section shortcut_6\n",
      "Parsing section convolutional_18\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_19\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section shortcut_7\n",
      "Parsing section convolutional_20\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_21\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section shortcut_8\n",
      "Parsing section convolutional_22\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_23\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section shortcut_9\n",
      "Parsing section convolutional_24\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_25\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section shortcut_10\n",
      "Parsing section convolutional_26\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section convolutional_27\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_28\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section shortcut_11\n",
      "Parsing section convolutional_29\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_30\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section shortcut_12\n",
      "Parsing section convolutional_31\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_32\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section shortcut_13\n",
      "Parsing section convolutional_33\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_34\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section shortcut_14\n",
      "Parsing section convolutional_35\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_36\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section shortcut_15\n",
      "Parsing section convolutional_37\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_38\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section shortcut_16\n",
      "Parsing section convolutional_39\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_40\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section shortcut_17\n",
      "Parsing section convolutional_41\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_42\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section shortcut_18\n",
      "Parsing section convolutional_43\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section convolutional_44\n",
      "conv2d bn leaky (1, 1, 1024, 512)\n",
      "Parsing section convolutional_45\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section shortcut_19\n",
      "Parsing section convolutional_46\n",
      "conv2d bn leaky (1, 1, 1024, 512)\n",
      "Parsing section convolutional_47\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section shortcut_20\n",
      "Parsing section convolutional_48\n",
      "conv2d bn leaky (1, 1, 1024, 512)\n",
      "Parsing section convolutional_49\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section shortcut_21\n",
      "Parsing section convolutional_50\n",
      "conv2d bn leaky (1, 1, 1024, 512)\n",
      "Parsing section convolutional_51\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section shortcut_22\n",
      "Parsing section convolutional_52\n",
      "conv2d bn leaky (1, 1, 1024, 512)\n",
      "Parsing section convolutional_53\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section convolutional_54\n",
      "conv2d bn leaky (1, 1, 1024, 512)\n",
      "Parsing section convolutional_55\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section convolutional_56\n",
      "conv2d bn leaky (1, 1, 1024, 512)\n",
      "Parsing section convolutional_57\n",
      "conv2d bn leaky (3, 3, 512, 1024)\n",
      "Parsing section convolutional_58\n",
      "conv2d    linear (1, 1, 1024, 255)\n",
      "Parsing section yolo_0\n",
      "Parsing section route_0\n",
      "Parsing section convolutional_59\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section upsample_0\n",
      "Parsing section route_1\n",
      "Concatenating route layers: [<tf.Tensor 'up_sampling2d/Identity:0' shape=(None, None, None, 256) dtype=float32>, <tf.Tensor 'add_18/Identity:0' shape=(None, None, None, 512) dtype=float32>]\n",
      "Parsing section convolutional_60\n",
      "conv2d bn leaky (1, 1, 768, 256)\n",
      "Parsing section convolutional_61\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section convolutional_62\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_63\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section convolutional_64\n",
      "conv2d bn leaky (1, 1, 512, 256)\n",
      "Parsing section convolutional_65\n",
      "conv2d bn leaky (3, 3, 256, 512)\n",
      "Parsing section convolutional_66\n",
      "conv2d    linear (1, 1, 512, 255)\n",
      "Parsing section yolo_1\n",
      "Parsing section route_2\n",
      "Parsing section convolutional_67\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section upsample_1\n",
      "Parsing section route_3\n",
      "Concatenating route layers: [<tf.Tensor 'up_sampling2d_1/Identity:0' shape=(None, None, None, 128) dtype=float32>, <tf.Tensor 'add_10/Identity:0' shape=(None, None, None, 256) dtype=float32>]\n",
      "Parsing section convolutional_68\n",
      "conv2d bn leaky (1, 1, 384, 128)\n",
      "Parsing section convolutional_69\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section convolutional_70\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_71\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section convolutional_72\n",
      "conv2d bn leaky (1, 1, 256, 128)\n",
      "Parsing section convolutional_73\n",
      "conv2d bn leaky (3, 3, 128, 256)\n",
      "Parsing section convolutional_74\n",
      "conv2d    linear (1, 1, 256, 255)\n",
      "Parsing section yolo_2\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, None, None, 3 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, None, 3 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, None, None, 3 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, None, None, 3 0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 6 18432       zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 6 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 3 2048        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 3 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, None, None, 3 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 6 18432       leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 6 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, None, None, 6 0           leaky_re_lu_1[0][0]              \n",
      "                                                                 leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, None, None, 6 0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 1 73728       zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 1 512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 6 8192        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 6 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 1 73728       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 1 512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, None, 1 0           leaky_re_lu_4[0][0]              \n",
      "                                                                 leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 6 8192        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 6 256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 1 73728       leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 1 512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, None, None, 1 0           add_1[0][0]                      \n",
      "                                                                 leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, None, None, 1 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 2 294912      zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 2 1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, None, None, 2 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 1 512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, 2 1024        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, None, None, 2 0           leaky_re_lu_9[0][0]              \n",
      "                                                                 leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 1 32768       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, None, 1 512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, None, 2 1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, None, None, 2 0           add_3[0][0]                      \n",
      "                                                                 leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 1 32768       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, None, 1 512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, None, 2 1024        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, None, None, 2 0           add_4[0][0]                      \n",
      "                                                                 leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 1 32768       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, None, 1 512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, None, 2 1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, None, None, 2 0           add_5[0][0]                      \n",
      "                                                                 leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, None, None, 1 32768       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, None, 1 512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, None, 2 1024        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, None, None, 2 0           add_6[0][0]                      \n",
      "                                                                 leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, None, None, 1 32768       add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, None, 1 512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, None, 2 1024        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, None, None, 2 0           add_7[0][0]                      \n",
      "                                                                 leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, None, None, 1 32768       add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, None, 1 512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, None, 2 1024        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, None, None, 2 0           add_8[0][0]                      \n",
      "                                                                 leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, None, None, 1 32768       add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, None, None, 1 512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, None, None, 2 1024        conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, None, None, 2 0           add_9[0][0]                      \n",
      "                                                                 leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, None, None, 2 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, None, None, 5 1179648     zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, None, None, 5 2048        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, None, None, 2 1024        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, None, None, 5 2048        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, None, None, 5 0           leaky_re_lu_26[0][0]             \n",
      "                                                                 leaky_re_lu_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, None, None, 2 131072      add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, None, None, 2 1024        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, None, None, 5 2048        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, None, None, 5 0           add_11[0][0]                     \n",
      "                                                                 leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, None, None, 2 131072      add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, None, 2 1024        conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, None, 5 2048        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, None, None, 5 0           add_12[0][0]                     \n",
      "                                                                 leaky_re_lu_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, None, None, 2 131072      add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, None, 2 1024        conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, None, 5 2048        conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, None, None, 5 0           add_13[0][0]                     \n",
      "                                                                 leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, None, None, 2 131072      add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, None, 2 1024        conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, None, 5 2048        conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, None, None, 5 0           add_14[0][0]                     \n",
      "                                                                 leaky_re_lu_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, None, None, 2 131072      add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, None, None, 2 1024        conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, None, None, 5 2048        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, None, None, 5 0           add_15[0][0]                     \n",
      "                                                                 leaky_re_lu_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, None, None, 2 131072      add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, None, None, 2 1024        conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, None, None, 5 2048        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, None, None, 5 0           add_16[0][0]                     \n",
      "                                                                 leaky_re_lu_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, None, None, 2 131072      add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, None, None, 2 1024        conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, None, None, 5 2048        conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, None, None, 5 0           add_17[0][0]                     \n",
      "                                                                 leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, None, None, 5 0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, None, None, 1 4718592     zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, None, None, 1 4096        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, None, None, 5 524288      leaky_re_lu_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, None, None, 5 2048        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, None, None, 1 4096        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, None, None, 1 0           leaky_re_lu_43[0][0]             \n",
      "                                                                 leaky_re_lu_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, None, None, 5 524288      add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, None, None, 5 2048        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, None, None, 1 4096        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, None, None, 1 0           add_19[0][0]                     \n",
      "                                                                 leaky_re_lu_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, None, None, 5 524288      add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, None, None, 5 2048        conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, None, None, 1 4096        conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, None, None, 1 0           add_20[0][0]                     \n",
      "                                                                 leaky_re_lu_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, None, None, 5 524288      add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, None, None, 5 2048        conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, None, None, 1 4096        conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, None, None, 1 0           add_21[0][0]                     \n",
      "                                                                 leaky_re_lu_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, None, None, 5 524288      add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, None, None, 5 2048        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, None, None, 1 4096        conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_53 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, None, None, 5 524288      leaky_re_lu_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, None, None, 5 2048        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_54 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, None, None, 1 4096        conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_55 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, None, None, 5 524288      leaky_re_lu_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, None, None, 5 2048        conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_56 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, None, None, 2 1024        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_58 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, None, None, 2 0           leaky_re_lu_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, None, 7 0           up_sampling2d[0][0]              \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, None, None, 2 196608      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, None, None, 2 1024        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, None, None, 5 2048        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_60[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, None, None, 2 1024        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_61 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_61[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, None, None, 5 2048        conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_62 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, None, None, 2 131072      leaky_re_lu_62[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, None, None, 2 1024        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, None, None, 1 512         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, None, None, 1 0           leaky_re_lu_65[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 3 0           up_sampling2d_1[0][0]            \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, None, None, 1 49152       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, None, None, 1 512         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_66[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, None, None, 2 1024        conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, None, None, 1 512         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, None, None, 2 1024        conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_69[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, None, None, 1 512         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, None, None, 1 4718592     leaky_re_lu_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, None, None, 5 1179648     leaky_re_lu_63[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, None, None, 2 294912      leaky_re_lu_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, None, None, 1 4096        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, None, None, 5 2048        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, None, None, 2 1024        conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, None, None, 2 261375      leaky_re_lu_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, None, None, 2 130815      leaky_re_lu_64[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, None, None, 2 65535       leaky_re_lu_71[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 62,001,757\n",
      "Trainable params: 61,949,149\n",
      "Non-trainable params: 52,608\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Saved Keras model to model_data/yolo.h5\n",
      "Read 62001757 of 62001757.0 from Darknet weights.\n"
     ]
    }
   ],
   "source": [
    "# call a Python script to set up our architecture with downloaded pre-trained weights\n",
    "!python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BEDHwJ36YyXA"
   },
   "source": [
    "Below, we'll call a \"self-contained\" Python script that initiates training our model on our custom dataset.\n",
    "\n",
    "Pay notable attention to:\n",
    "- setting the paths for our `annotation_path`, `classes_path`, `class_names`. If you move the Roboflow data location, you'll need to update these. \n",
    "- `val_split` dictates the size of our training data relative to our taining data\n",
    "- `lr=1e-3` to set the learning rate of the model. Smaller optimizes more slowly but potentially more precisely.\n",
    "- `batch_size` for the number of images trained per batch\n",
    "-  `epoch` inside `model.fit_generator()` sets the number training epochs to increase/decrease training examples (and time)\n",
    "\n",
    "Consider reading the YOLOv3 paper [here](https://pjreddie.com/media/files/papers/YOLOv3.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "colab_type": "code",
    "id": "4hBFndz8VeI6",
    "outputId": "286072a9-b6ae-4b57-fc58-069e85287580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------CLASS NAMES-------------------\n",
      "['crosswalk', 'pedestrian_traffic_light', 'traffic_light']\n",
      "-------------------CLASS NAMES-------------------\n",
      "Create YOLOv3 model with 9 anchors and 3 classes.\n",
      "WARNING:tensorflow:Skipping loading of weights for layer conv2d_58 due to mismatch in shape ((1, 1, 1024, 24) vs (255, 1024, 1, 1)).\n",
      "WARNING:tensorflow:Skipping loading of weights for layer conv2d_58 due to mismatch in shape ((24,) vs (255,)).\n",
      "WARNING:tensorflow:Skipping loading of weights for layer conv2d_66 due to mismatch in shape ((1, 1, 512, 24) vs (255, 512, 1, 1)).\n",
      "WARNING:tensorflow:Skipping loading of weights for layer conv2d_66 due to mismatch in shape ((24,) vs (255,)).\n",
      "WARNING:tensorflow:Skipping loading of weights for layer conv2d_74 due to mismatch in shape ((1, 1, 256, 24) vs (255, 256, 1, 1)).\n",
      "WARNING:tensorflow:Skipping loading of weights for layer conv2d_74 due to mismatch in shape ((24,) vs (255,)).\n",
      "Load weights model_data/yolo.h5.\n",
      "Freeze the first 249 layers of total 252 layers.\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "slice index -1 of dimension 0 out of bounds. for 'loss/yolo_loss_loss/strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <-1>, input[2] = <0>, input[3] = <1>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1618\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1620\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: slice index -1 of dimension 0 out of bounds. for 'loss/yolo_loss_loss/strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <-1>, input[2] = <0>, input[3] = <1>.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1643eeba586c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0m_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-1643eeba586c>\u001b[0m in \u001b[0;36m_main\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         model.compile(optimizer=Adam(lr=1e-3), loss={\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# use custom yolo_loss Lambda layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             'yolo_loss': lambda y_true, y_pred: y_pred})\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m       \u001b[0;31m# Creates the model loss and weighted metrics sub-graphs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_weights_loss_and_weighted_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m       \u001b[0;31m# Functions for train, test and predict will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_compile_weights_loss_and_weighted_metrics\u001b[0;34m(self, sample_weights)\u001b[0m\n\u001b[1;32m   1590\u001b[0m       \u001b[0;31m#                   loss_weight_2 * output_2_loss_fn(...) +\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m       \u001b[0;31m#                   layer losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_total_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_skip_target_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_prepare_total_loss\u001b[0;34m(self, masks)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reduction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m             \u001b[0mper_sample_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m             weighted_losses = losses_utils.compute_weighted_loss(\n\u001b[1;32m   1654\u001b[0m                 \u001b[0mper_sample_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/losses.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\n\u001b[0;32m--> 220\u001b[0;31m           y_pred, y_true)\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/util.py\u001b[0m in \u001b[0;36msqueeze_or_expand_dimensions\u001b[0;34m(y_pred, y_true, sample_weight)\u001b[0m\n\u001b[1;32m     77\u001b[0m       squeeze_dims = lambda: confusion_matrix.remove_squeezable_dimensions(  # pylint: disable=g-long-lambda\n\u001b[1;32m     78\u001b[0m           y_true, y_pred)\n\u001b[0;32m---> 79\u001b[0;31m       \u001b[0mis_last_dim_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m       maybe_squeeze_dims = lambda: control_flow_ops.cond(  # pylint: disable=g-long-lambda\n\u001b[1;32m     81\u001b[0m           is_last_dim_1, squeeze_dims, lambda: (y_true, y_pred))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m   9533\u001b[0m                         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9534\u001b[0m                         \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9535\u001b[0;31m                         shrink_axis_mask=shrink_axis_mask, name=name)\n\u001b[0m\u001b[1;32m   9536\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9537\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    740\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[1;32m    741\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    593\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[1;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         compute_device)\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3320\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3321\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3322\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3323\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3324\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1784\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1785\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1786\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1787\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1620\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1622\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: slice index -1 of dimension 0 out of bounds. for 'loss/yolo_loss_loss/strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <-1>, input[2] = <0>, input[3] = <1>."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Self-contained Python script to train YOLOv3 on your own dataset\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
    "from yolo3.utils import get_random_data\n",
    "\n",
    "\n",
    "def _main():\n",
    "    annotation_path = 'sample_data/train/_annotations.txt'  # path to Roboflow data annotations\n",
    "    log_dir = 'logs/000/'                 # where we're storing our logs\n",
    "    classes_path = 'sample_data/train/_classes.txt'         # path to Roboflow class names\n",
    "    anchors_path = 'model_data/yolo_anchors.txt'\n",
    "    class_names = get_classes(classes_path)\n",
    "    print(\"-------------------CLASS NAMES-------------------\")\n",
    "    print(class_names)\n",
    "    print(\"-------------------CLASS NAMES-------------------\")\n",
    "    num_classes = len(class_names)\n",
    "    anchors = get_anchors(anchors_path)\n",
    "\n",
    "    input_shape = (416,416) # multiple of 32, hw\n",
    "\n",
    "    is_tiny_version = len(anchors)==6 # default setting\n",
    "    if is_tiny_version:\n",
    "        model = create_tiny_model(input_shape, anchors, num_classes,\n",
    "            freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')\n",
    "    else:\n",
    "        model = create_model(input_shape, anchors, num_classes,\n",
    "            freeze_body=2, weights_path='model_data/yolo.h5') # make sure you know what you freeze\n",
    "\n",
    "    logging = TensorBoard(log_dir=log_dir)\n",
    "    checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
    "\n",
    "    val_split = 0.2 # set the size of the validation set\n",
    "    with open(annotation_path) as f:\n",
    "        lines = f.readlines()\n",
    "    np.random.seed(10101)\n",
    "    np.random.shuffle(lines)\n",
    "    np.random.seed(None)\n",
    "    num_val = int(len(lines)*val_split)\n",
    "    num_train = len(lines) - num_val\n",
    "\n",
    "    # Train with frozen layers first, to get a stable loss.\n",
    "    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
    "    if True:\n",
    "        model.compile(optimizer=Adam(lr=1e-3), loss={\n",
    "            # use custom yolo_loss Lambda layer.\n",
    "            'yolo_loss': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "        batch_size = 32\n",
    "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "                steps_per_epoch=max(1, num_train//batch_size),\n",
    "                validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "                validation_steps=max(1, num_val//batch_size),\n",
    "                epochs=500,\n",
    "                initial_epoch=0,\n",
    "                callbacks=[logging, checkpoint])\n",
    "        model.save_weights(log_dir + 'trained_weights_stage_1.h5')\n",
    "\n",
    "    # Unfreeze and continue training, to fine-tune.\n",
    "    # Train longer if the result is not good.\n",
    "    if True:\n",
    "        for i in range(len(model.layers)):\n",
    "            model.layers[i].trainable = True\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
    "        print('Unfreeze all of the layers.')\n",
    "\n",
    "        batch_size = 32 # note that more GPU memory is required after unfreezing the body\n",
    "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "            steps_per_epoch=max(1, num_train//batch_size),\n",
    "            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "            validation_steps=max(1, num_val//batch_size),\n",
    "            epochs=100,\n",
    "            initial_epoch=50,\n",
    "            callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n",
    "        model.save_weights(log_dir + 'trained_weights_final.h5')\n",
    "\n",
    "    # Further training if needed.\n",
    "\n",
    "\n",
    "def get_classes(classes_path):\n",
    "    '''loads the classes'''\n",
    "    with open(classes_path) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names\n",
    "\n",
    "def get_anchors(anchors_path):\n",
    "    '''loads the anchors from a file'''\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "\n",
    "def create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
    "            weights_path='model_data/yolo.h5'):\n",
    "    '''create the training model'''\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n",
    "        num_anchors//3, num_classes+5)) for l in range(3)]\n",
    "\n",
    "    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n",
    "    print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
    "            num = (185, len(model_body.layers)-3)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
    "            weights_path='model_data/tiny_yolo_weights.h5'):\n",
    "    '''create the training model, for Tiny YOLOv3'''\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\n",
    "        num_anchors//2, num_classes+5)) for l in range(2)]\n",
    "\n",
    "    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n",
    "    print('Create Tiny YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze the darknet body or freeze all but 2 output layers.\n",
    "            num = (20, len(model_body.layers)-2)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.7})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    '''data generator for fit_generator'''\n",
    "    n = len(annotation_lines)\n",
    "    i = 0\n",
    "    while True:\n",
    "        image_data = []\n",
    "        box_data = []\n",
    "        for b in range(batch_size):\n",
    "            if i==0:\n",
    "                np.random.shuffle(annotation_lines)\n",
    "            image, box = get_random_data(annotation_lines[i], input_shape, random=True)\n",
    "            image_data.append(image)\n",
    "            box_data.append(box)\n",
    "            i = (i+1) % n\n",
    "        image_data = np.array(image_data)\n",
    "        box_data = np.array(box_data)\n",
    "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
    "        yield [image_data, *y_true], np.zeros(batch_size)\n",
    "\n",
    "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    n = len(annotation_lines)\n",
    "    if n==0 or batch_size<=0: return None\n",
    "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    _main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48yw4UaOYgQS"
   },
   "outputs": [],
   "source": [
    "## can call this cell instead of the above\n",
    "# !python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFX-2_M8bMQ3"
   },
   "source": [
    "## Use our model for inference\n",
    "\n",
    "For predictions, we'll call a a Python script called `yolo_video.py` with required arguments for our use case: a path to our specific first stage trained weights (see our blog for why we're using only stage one), a path to our custom class names, and a flag to specify we're using images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AlVyevd8b8gG"
   },
   "source": [
    "Additional arguments for `yolo_video.py` are as follows:\n",
    "\n",
    "```\n",
    "usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]\n",
    "                     [--classes CLASSES] [--gpu_num GPU_NUM] [--image]\n",
    "                     [--input] [--output]\n",
    "\n",
    "positional arguments:\n",
    "  --input        Video input path\n",
    "  --output       Video output path\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help         show this help message and exit\n",
    "  --model MODEL      path to model weight file, default model_data/yolo.h5\n",
    "  --anchors ANCHORS  path to anchor definitions, default\n",
    "                     model_data/yolo_anchors.txt\n",
    "  --classes CLASSES  path to class definitions, default\n",
    "                     model_data/coco_classes.txt\n",
    "  --gpu_num GPU_NUM  Number of GPU to use, default 1\n",
    "  --image            Image detection mode, will ignore all positional arguments\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zcJbmgNEO1bE"
   },
   "outputs": [],
   "source": [
    "!python yolo_video.py --model=\"./logs/000/trained_weights_stage_1.h5\" --classes=\"_classes.txt\" --image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EbACT_RJdGVg"
   },
   "source": [
    "For input image names into the above, consider trying the following:\n",
    "\n",
    "- `00a7a49c47d51fd16a4cbb17e2d2cf86.jpg` # white-king works! + knight\n",
    "- `015d0d7ff365f0b7492ff079c8c7d56c.jpg` # black-queen mixes up\n",
    "- `176b28b5c417f39a9e5d37545fca5b4c.jpg` # finds only five\n",
    "- `4673f994f60a2ea7afdddc1b752947c0.jpg` # white-rook (thinks king)\n",
    "- `5ca7f0cb1c500554e65ad031190f8e9f.jpg` # white-pawn (missed white-king)\n",
    "- `fbf15139f38a46e02b5f4061c0c9b08f.jpg` # black-king success!\n",
    "\n",
    "You can view these images in your Colab notebook by clicking on the image name in the expanded left-hand panel (Files â†’ keras-yolo3 â†’ IMG_NAME )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88oJlBl4dumo"
   },
   "source": [
    "## Move currently trained model to GDrive\n",
    "\n",
    "Optionally, you may want to save the new weights that your model trained so that the next time you run this notebook, you can either skip training and use these weights for inference or begin training where you left off with this weights file.\n",
    "\n",
    "Following the below will link your Colab notebook to your Google Drive, and save the weights (named as the current time you saved them to enforce a unique file name) in your Drive folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4t94dBNdsxz"
   },
   "outputs": [],
   "source": [
    "# mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLe0Y4Z8BOVF"
   },
   "outputs": [],
   "source": [
    "# create a copy of the weights file with a datetime \n",
    "# and move that file to your own Drive\n",
    "%cp ./logs/000/trained_weights_stage_1.h5 ./logs/000/trained_weights_stage_1_$(date +%F-%H:%M).h5\n",
    "%mv ./logs/000/trained_weights_stage_1_$(date +%F-%H:%M).h5 /content/drive/My\\ Drive/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Roboflow-Yolov3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
